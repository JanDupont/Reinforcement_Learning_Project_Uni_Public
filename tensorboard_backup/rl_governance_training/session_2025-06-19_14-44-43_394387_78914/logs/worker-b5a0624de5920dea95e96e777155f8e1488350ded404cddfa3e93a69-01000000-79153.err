:job_id:01000000
:actor_name:RolloutWorker
2025-06-19 14:44:55,556	INFO catalog.py:407 -- Wrapping <class 'ray_models.GovernanceModel'> as None
2025-06-19 14:44:55,558	INFO policy.py:1234 -- Policy (worker=1) running on CPU.
2025-06-19 14:44:55,558	INFO torch_policy_v2.py:105 -- Found 0 visible cuda devices.
2025-06-19 14:44:56,356	INFO catalog.py:407 -- Wrapping <class 'ray_models.PistonParametricModel'> as None
2025-06-19 14:44:56,485	INFO policy.py:1234 -- Policy (worker=1) running on CPU.
2025-06-19 14:44:56,485	INFO torch_policy_v2.py:105 -- Found 0 visible cuda devices.
2025-06-19 14:44:56,886	INFO util.py:118 -- Using connectors:
2025-06-19 14:44:56,886	INFO util.py:119 --     AgentConnectorPipeline
        ObsPreprocessorConnector
        StateBufferConnector
        ViewRequirementAgentConnector
2025-06-19 14:44:56,886	INFO util.py:120 --     ActionConnectorPipeline
        ConvertToNumpyConnector
        NormalizeActionsConnector
        ImmutableActionsConnector
2025-06-19 14:44:56,889	INFO util.py:118 -- Using connectors:
2025-06-19 14:44:56,889	INFO util.py:119 --     AgentConnectorPipeline
        ObsPreprocessorConnector
        StateBufferConnector
        ViewRequirementAgentConnector
2025-06-19 14:44:56,889	INFO util.py:120 --     ActionConnectorPipeline
        ConvertToNumpyConnector
        NormalizeActionsConnector
        ImmutableActionsConnector
2025-06-19 14:44:58,115	INFO rollout_worker.py:671 -- Generating sample batch of size 250
2025-06-19 14:45:15,351	WARNING env_runner_v2.py:157 -- More than 2750 observations in 250 env steps for episode 561485256495590669 are buffered in the sampler. If this is more than you expected, check that that you set a horizon on your environment correctly and that it terminates at some point. Note: In multi-agent environments, `rollout_fragment_length` sets the batch size based on (across-agents) environment steps, not the steps of individual agents, which can result in unexpectedly large batches.
2025-06-19 14:45:16,787	INFO rollout_worker.py:713 -- Completed sample batch:

{ 'count': 250,
  'policy_batches': { 'governance': { 'action_dist_inputs': np.ndarray((250, 20), dtype=float32, min=-0.075, max=0.137, mean=0.013),
                                      'action_logp': np.ndarray((250,), dtype=float32, min=-21.873, max=-10.103, mean=-13.904),
                                      'actions': np.ndarray((250, 10), dtype=float32, min=-3.466, max=3.232, mean=0.033),
                                      'advantages': np.ndarray((250,), dtype=float32, min=-83.143, max=260.866, mean=21.403),
                                      'agent_index': np.ndarray((250,), dtype=int64, min=10.0, max=10.0, mean=10.0),
                                      'eps_id': np.ndarray((250,), dtype=int64, min=5.6148525649559066e+17, max=6.522603866799724e+17, mean=6.072359221085189e+17),
                                      'infos': np.ndarray((250,), dtype=object, head={}),
                                      'new_obs': np.ndarray((250, 1000), dtype=float32, min=0.0, max=77.0, mean=71.335),
                                      'obs': np.ndarray((250, 1000), dtype=float32, min=58.0, max=77.0, mean=71.621),
                                      'rewards': np.ndarray((250,), dtype=float32, min=-58.823, max=107.844, mean=0.936),
                                      't': np.ndarray((250,), dtype=int64, min=0.0, max=125.0, mean=62.004),
                                      'terminateds': np.ndarray((250,), dtype=bool, min=0.0, max=1.0, mean=0.004),
                                      'truncateds': np.ndarray((250,), dtype=bool, min=0.0, max=1.0, mean=0.004),
                                      'unroll_id': np.ndarray((250,), dtype=int64, min=21.0, max=43.0, mean=31.912),
                                      'value_targets': np.ndarray((250,), dtype=float32, min=-83.251, max=260.758, mean=21.295),
                                      'values_bootstrapped': np.ndarray((250,), dtype=float32, min=-0.108, max=0.0, mean=-0.107),
                                      'vf_preds': np.ndarray((250,), dtype=float32, min=-0.108, max=-0.108, mean=-0.108)},
                      'piston': { 'action_dist_inputs': np.ndarray((2500, 2), dtype=float32, min=0.006, max=0.034, mean=0.02),
                                  'action_logp': np.ndarray((2500,), dtype=float32, min=-7.847, max=-0.952, mean=-1.471),
                                  'actions': np.ndarray((2500, 1), dtype=float32, min=-3.834, max=3.561, mean=-0.036),
                                  'advantages': np.ndarray((2500,), dtype=float32, min=-8.315, max=26.086, mean=2.14),
                                  'agent_index': np.ndarray((2500,), dtype=int64, min=0.0, max=9.0, mean=4.5),
                                  'eps_id': np.ndarray((2500,), dtype=int64, min=5.6148525649559066e+17, max=6.522603866799724e+17, mean=6.072359221085189e+17),
                                  'infos': np.ndarray((2500,), dtype=object, head={}),
                                  'new_obs': np.ndarray((2500, 457, 120, 3), dtype=uint8, min=58.0, max=221.0, mean=74.185),
                                  'obs': np.ndarray((2500, 457, 120, 3), dtype=uint8, min=58.0, max=221.0, mean=74.185),
                                  'rewards': np.ndarray((2500,), dtype=float32, min=-5.882, max=10.784, mean=0.094),
                                  't': np.ndarray((2500,), dtype=int64, min=0.0, max=125.0, mean=62.004),
                                  'terminateds': np.ndarray((2500,), dtype=bool, min=0.0, max=1.0, mean=0.004),
                                  'truncateds': np.ndarray((2500,), dtype=bool, min=0.0, max=1.0, mean=0.004),
                                  'unroll_id': np.ndarray((2500,), dtype=int64, min=10.0, max=41.0, mean=25.412),
                                  'value_targets': np.ndarray((2500,), dtype=float32, min=-8.323, max=26.079, mean=2.132),
                                  'values_bootstrapped': np.ndarray((2500,), dtype=float32, min=-0.008, max=0.0, mean=-0.008),
                                  'vf_preds': np.ndarray((2500,), dtype=float32, min=-0.008, max=-0.007, mean=-0.008)}},
  'type': 'MultiAgentBatch'}

